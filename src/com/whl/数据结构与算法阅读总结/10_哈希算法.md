Hash算法的原理就是将任意长度的二进制串映射为固定长度的二进制串。一个优秀的Hash算法具备如下特征：

* 从哈希值不能反推原始数据
* 对输入数据非常敏感，一点修改就会导致Hash值大不相同
* 散列冲突的概率要小
* 算法的执行效率要尽可能高效

例如，对如下两个文本进行MD5哈希算法的加密，无论文本有多长、多短，通过MD5哈希之后得到的值长度都是相同的：

```
MD5("今天我来讲哈希算法") = bb4767201ad42c74e650c1b6c03d78fa
MD5("jiajia") = cd611a31ea969b908932d44d126d195b
```

且一点改动都会导致Hash值大不相同：

```
MD5("我今天讲哈希算法！") = 425f0d5a917188d2c3c3dc85b5e4f2cb
MD5("我今天讲哈希算法") = a1fb91ac128e6aa37fe42c663971ac3d
```

虽然网络上有相当多的哈希解密网站，但MD5依然属于较难反向推导的哈希算法。

哈希算法的应用广泛，常见的场景有：安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储。



### 安全加密

安全加密场景下最常用的算法是MD5、SHA等。在满足无法反向推导原始数据的基础上，我们还需要使得Hash冲突概率尽量小。

**为什么哈希算法无法做到0冲突：**

例如MD5算法，哈希值固定是128位的二进制字符串，能表示的数据最多只能有2^128个，而我们要哈希的数据是无穷的，这就必定存在哈希值相同的情况。一般情况下，哈希值越长的哈希算法，散列冲突概率越低。

除此以外，假如我是黑客，并盗窃了一组被Hash算法加密的密码。那么我可以维护一个常用密码表，然后将密码表通过Hash算法获取到的所有值与盗窃的密码一一比对，时间复杂度为O(n^2)，如果匹配上了，那么就成功破译了，具体伪代码如下所示：

```java
for (String myUsualPswdTb : MyUsualPswdTb) {//被Hash后的密码表集合
	for (String userPswdTb : UserPswdTb) {//盗窃的用户密码
    	if(myUsualPswdTb.equals(userPswdTb)) {
        	//记录下当前的myUsualPswdTb
            //去常用密码表中寻找对应密文的明文即可完成破译
        }
    }
}
```

那么为了避免这种攻击手段，我们可以引入一个 “盐” ，它与用户密码结合，增加密码的复杂度，避免出现简单的密文被破解的情况。



### 唯一标识

当我们需要在海量图库中搜索某图片是否存在，我们不能单纯地用图片的原信息（例如名称）来比对，因为有可能存在相同名称的不同图片；或相同图片但不同名称的情况。而任何文件在计算器中都能表示为二进制串，因此最简单的方法就是拿待查找图片的二进制串与图库中所有图片的二进制串匹配，但这个方法效率是非常低的。

其实我们可以给每一个图片取一个唯一标识，例如将图片二进制串开头取100个字节，中间取100个字节，末尾取100个字节，再将300个字节放在一起通过哈希算法获取到一个Hash字符串，用于作为唯一标识。我们只需要通过这个唯一标识就能够判断图片是否存在于库中。



### 数据校验

当我们使用BT下载软件时，其原理是基于P2P协议的，我们从多个机器上并行下载一个2Gb的电影，这些电影文件可能被分为很多个文件块，等到所有的文件块都下载完成之后，再组装为一个完整的电影文件即可。

但网络传输是不安全的，下载的文件有可能被恶意修改或是下载出错，导致文件块不完整。为此我们需要通过某个方式对下载的文件块进行校验：我们通过哈希算法，对100个文件块分别取哈希值，并且保存在种子文件中。当文件块都下载好了之后，再通过相同的哈希算法对下载好的文件块逐一求值，然后将结果和种子文件中的哈希值比对。若不同，则可以确定文件块不完整或被篡改。



### 散列函数

散列函数相比起其他哈希算法应用，对散列算法冲突的要求要低很多。即便个别出现散列冲突，也可以通过开放寻址法或拉链法解决。除此以外，散列函数相比起 “反向推导原始值” ，更加关注的是 “一组数据是否能均匀分布在各个桶”，并且哈希算法的性能也需要追求极致。



### 负载均衡

如果我们需要将同一个客户端的一次会话中的所有请求都路由到同一个服务器上，最直接的办法就是维护一个映射表，内容是客户端IP地址与服务器编号的映射关系。每当客户端发一个请求，就去映射表中获取到对应的服务器编号，然后再请求编号对应的服务器即可。但这种方式缺点如下：

1. 如果客户端很多的话，映射表可能会很大，比较浪费内存空间
2. 客户端上、下线以及服务器扩容、缩容都会导致映射失效，这样维护成本就较大。

如果借助Hash算法，对客户端IP计算Hash值，将Hash值与服务器列表大小取模运算，最终得到的值就是被路由到的服务器编号。这样就可以把同一个IP的请求都路由到同一个后端服务器上。



### 数据分片

**1. 统计关键词出现频次：**

假设有1T的日志文件，记录了用户的搜索关键词，如果我们想要快速统计出每个关键词被搜索的次数，该怎么做。首先可以确定1T的文件是无法一次性放到同一个机器下进行处理的；其次如果只用一台机器处理，那么时间也会很长。

我们用多台机器并行处理，从搜索记录的日志中依次读取每个搜索关键词，再计算关键词的Hash值，将Hash值与机器台数n取模，最终获取到的值就是被分配的机器编号。这样，同一个搜索关键词就被分配到了同一个机器上，每个机器会并行计算关键词出现的次数，最终将所有机器的执行结果合并起来就是总的结果。

上述处理思路也是MapReduce的基本思想。



**2. 如何快速判断图片是否在图库中：**

在前面的 **唯一标识** 中提到了：可以将图库中的所有图片都通过Hash算法计算一个唯一标识，然后将待搜索图片的唯一标识去逐个匹配，就能够快速判断图片是否在图库中。

但如果图库中有1e张图片，那么在单台机器上构建散列表索引（key = 唯一标识，value = 图片）是行不通的。我们同样可以对数据分片，采用n台机器处理，每台机器只维护一部分图片对应的散列表索引。当我们检索某张图片是否存在库中时，先对其计算唯一表示，然后与机器个数n取模，得到的值就是对应的机器编号，然后拿这个图片的唯一标识去到对应机器编号的散列表索引处匹配。



### 分布式存储

我们为了提高数据的读写能力，一般都是采用分布式存储数据，比如分布式缓存 —— 将数据分布在多台机器上。我们可以借助前面数据分片的思想，通过Hash算法对数据取hash值，然后对机器个数取模获取对应缓存的机器编号。

但是随着数据的增多，我们需要从原本的10个机器扩容到11个机器，而这里就可能引发 “缓存雪崩” 的问题。之前的数据是通过10取模，例如13这个数据原本是被分配到了3号机上，但是在扩容到11台机器之后，13这个数据就被分配到了2号机上。因此，所有的数据都需要重新计算Hash值，然后重新搬迁到正确的机器上，这就相当于缓存中的数据全都失效了。所有的数据都会穿透缓存，直接去数据库中获取，这就可能引发雪崩效应，导致压垮数据库。

因此，我们需要一致性Hash算法，使得在新加入一个机器之后，不需要进行大量的数据迁移动作。