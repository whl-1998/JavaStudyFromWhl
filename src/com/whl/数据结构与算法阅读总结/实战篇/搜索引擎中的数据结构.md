搜索引擎大致分为4个部分：搜集、分析、索引、查询。其中，搜集就是利用爬虫爬取网页；分析则主要负责网页内容抽取、分词、构建临时索引、计算PageRank值这几部分工作；索引则主要负责通过分析阶段获取的临时索引，从而构建倒排索引；查询则主要负责响应用户的请求，根据倒排索引获取相关网页，计算排名并返回查询结果。



### 一、搜集

如今互联网越来越庞大，网站也越来越多，对于搜索引擎来说，事先并不知道网页都在哪。那么搜索引擎该如何爬取网页呢？

搜索引擎把整个互联网看作有向图，每个页面作为一个顶点。如果某个页面中包含另一个页面的链接，那么就在两个顶点之间连一条有向边。我们可以利用图的遍历算法（BFS），遍历整个互联网中的网页。具体来说，搜索引擎就是通过广度优先搜索，先从权重比较高（知名度高）的链接开始，将其作为种子网页链接放入队列。爬虫按照DFS策略不停从队列中取出链接，然后爬取对应网页，再解析网页里包含的其他网页链接，再将解析出的链接放入队列继续遍历。

上述是搜集这个操作的大致原理，下面是一些关键的技术细节：



**1. 带爬取网页链接文件：links.bin**

在广度优先搜索爬取页面的过程中，爬虫会不停解析页面链接，将其放入队列。于是队列中的链接会越来越多，很可能多到内存放不下。因此，我们用一个存储在磁盘中的文件（links.bin）作为BFS中的队列，爬虫从links.bin文件中获取链接再爬取对应的页面，等爬取到网页之后再将解析出的链接存储到links.bin中。

通过这种方式来存储网页链接也带来了断点续爬的好处，当断电之后网页链接不会丢失，在重启之后还可以从之前爬取到的位置继续爬取。



**2. 如何解析页面获取链接：**

我们可以将页面看做一个大字符串，然后通过字符串匹配算法搜索到 < link > 标签，然后按顺序读取 < link > </ link > 之间的字符串，就能够获取到相应的网页链接了。



**3. 网页判重文件：bloom_filter.bin：**

使用布隆过滤器，就可以非常快速且节省内存地实现网页的判重。但问题是，如果布隆过滤器存在于内存中，当机器宕机重启后，布隆过滤器中的数据就被清空了，这就可能导致大量的重复爬取操作。

为此，我们可以定期（比如间隔半小时）将布隆过滤器持久化到磁盘，存储到 bloom_filter.bin 文件中。这样，即便出现宕机，也只会丢失部分数据，在重启后我们就可以重新读取磁盘中的 bloom_filter.bin 文件，将其恢复到内存中。



**4. 原始网页存储文件：doc_raw.bin：**

当爬取到网页之后，我们需要将其存储，以便于后续的离线分析、索引操作。但如果我们将每个网页都存储为一个独立的文件，那么磁盘中的文件就会非常多，而常用的文件系统显然不适合存储如此多的文件，因此我们可以把多个网页存储在一个文件中。每个网页之间通过一定的标识分割，方便后续读取。如下图所示：

![img](https://static001.geekbang.org/resource/image/19/4d/195c9a1dceaaa9f4d2483fa91455404d.jpg)

当然这样的一个文件也不能太大，因此可以设置每个文件的大小不能超过一定的值（例如1GB）。



**5. 网页链接以及编号对应的文件：doc_id.bin：**

在上图中我们提到了网页编号这个概念，它实际上就是给每个网页分配的一个唯一ID，方便后续对网页进行分析、索引。

我们可以按照网页被爬取的前后顺序，从小到大依次编号。例如，我们维护一个中心的计数器，每当爬取到一个网页，从计数器获取一个值分配给这个网页，然后计数器加一。在存储网页的同时，我们将网页链接与编号之间的对应关系存储在另一个 doc_id.bin 文件中。



### 二、分析

网页被爬取下来之后，需要对网页进行离线分析。分析主要包含如下两个步骤：抽取网页文本信息、分词并创建临时索引。

**1. 抽取网页文本信息：**

网页是半结构化数据，其中夹杂着各种标签，如 JS 代码、CSS样式等。而对于搜索引擎来说，只关心网页中的文本信息，那么我们可以对这部分信息进行抽离，大致可以分为如下两个步骤：

* 去除CSS、JS代码以及下拉框中的内容。也就是< style > </ style >、< script ></ script >、< option ></ option >这三组标签之间的内容。为此，我们可以利用AC自动机这种多模式串匹配算法，在网页这个大字符串中一次性查找< style > 、< script >、< option >这三个关键词。当找到某个关键词出现的位置之后，只需要依次往后遍历，直到遍历到结束标签为止。在这期间，遍历到的字符串就连带着标签从该网页中删除即可。
* 取出HTML标签，这一步也是通过字符串匹配算法实现，与上一步类似。



**2. 分词并创建临时索引：**

经过上述处理之后，就能够从网页中抽取核心文本了。接下来要对文本信息进行分词，并创建临时索引。

对于英文网页来说，分词非常简单，只需要通过空格、标点符号等分隔符，将每个单词分割开来即可。但对于中文网页来说，则比较复杂，其实现主要基于字典和规则的分词方法。其中，字典也称为词库，包含了大量常用词。我们借助词库并采用最常匹配原则（尽可能匹配更长的词）对文本进行分词。

例如，“中国人民解放了”，则匹配的不是“中国”、也不是“中国人”，而是匹配最长的“中国人民”。具体的底层实现则是基于Trie树，我们可以将词库中的所有词汇构建为Trie树结构，然后拿网页文本在Trie树中匹配获取结果即可。

每个网页的文本信息在分词完成之后都能获取到一组单词列表。我们将单词与网页之间的对应关系写入到一个临时的索引文件中（tmp_index.bin），这个文件则用于构建倒排索引文件：

![img](https://static001.geekbang.org/resource/image/15/1e/156ee98c0ad5763a082c1f3002d6051e.jpg)

在该文件中，我们存的是单词编号term_id，而并非单词本身，这样就可以节省tmp_index.bin文件的存储空间，而单词编号则采用计数器获取。在这个过程中我们还需要散列表记录已经编过号的单词，并存放到term_id.bin文件中。对网页文本信息分词的过程中，我们将分割的单词先去散列表中查找，如果找到则使用已经存在的编号；如果未找到则去计数器中获取编号，再存入散列表。

在整个网页处理（分词以及写入临时索引）完成后，我们将这个单词与编号之间的对应关系写入磁盘中，并命名为term_id.bin文件。

整个分析流程下来，我们就能够获取到临时索引文件 tmp_index.bin 以及单词编号文件 term_id.bin。



### 三、索引

索引阶段主要负责将分析阶段生成的临时索引文件构建为倒排索引。倒排索引中记录了每个单词以及它的网页列表：

![img](https://static001.geekbang.org/resource/image/de/34/de1f212bc669312a499bbbf2ee3a3734.jpg)

在临时索引文件中记录的是单词编号与网页编号之间的对应关系，那么如何通过临时索引构建倒排索引呢？

考虑到临时索引很大，无法一次性加载到内存，因此搜索引擎一般会采用多路归并排序来实现。我们先对临时索引文件按照单词编号大小进行排序，由于无法全部加载到内存，我们可以将其分割成多个小文件，对每个小文件独立排序最后再合并。

临时索引文件在排序完成之后，相同的单词就排列在一起了。我们只需要顺序地遍历排好序的临时索引文件，就能将每个单词对应的网页编号列表找出来，再将其存储到倒排索引文件中：

![img](https://static001.geekbang.org/resource/image/c9/e6/c91c960472d88233f60d5d4ce6538ee6.jpg)

除了倒排索引文件之外，我们还需要一个文件用于记录每个单词编号在倒排文件中的偏移位置。该文件命名为 term_offset.bin，用于帮助我们快速查找到某个单词编号在倒排索引中存储的位置，进而从倒排索引中读取到对应的网页编号列表。



### 四、查询

通过前三个阶段的铺垫，我们需要通过如下几个文件实现最终的用户搜索功能：

1. doc_id.bin：记录网页链接和编号之间的对应关系
2. term_id.bin：记录单词与编号之间的对应关系
3. index.bin：倒排索引，记录每个单词编号对应的网页列表关系
4. term_offset.bin：记录每个单词编号在倒排索引中的偏移位置

上述4个文件中，除了index.bin较大以外，其他的都较小。为了方便快速查找，我们将其他三个文件都存放在内存中，并将其组织成散列表这种数据结构。

当用户在搜索框中输入某个查询文本时，先对输入文本进行分词处理。假设分词处理过后能够获取到k个单词，我们就拿这k个单词去term_id.bin中获取到对应的单词编号。再用这k个单词编号去term_offset.bin中获取到对应单词编号在倒排索引中的偏移位置。再拿这k个偏移位置去倒排索引中获取到对应的网页列表。最后能获取到k个网页列表。

此时我们再针对这k个网页列表，统计列表中所有网页编号出现的次数。根据统计的结果，我们按照出现次数的多少从大到小排序，出现次数越多则表示包含越多的用户查询单词。

经过这一系列的操作，就能够获取到一组排好序的网页编号。我们只需要拿网页编号，去doc_id.bin查找对应的网页链接，分页显示给用户即可。